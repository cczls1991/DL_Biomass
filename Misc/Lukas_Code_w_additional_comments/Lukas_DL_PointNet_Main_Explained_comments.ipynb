{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb1d967-f9a8-42bd-b218-bd65be8b7c1a",
   "metadata": {},
   "source": [
    "Explaining Lukas Point Net ++ Regression Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af86ff-392c-4f08-94d4-a637402cbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "#Import classes defined in additional scripts\n",
    "from pn2_regressor import Net\n",
    "from pointcloud_dataset import PointCloudsInFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd390520-379d-4ef5-ac1c-3835430e2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Why do you have a write_las function?\n",
    "def write_las(outpoints, outfilepath, attribute_dict={}):\n",
    "    \"\"\"\n",
    "    :param outpoints: 3D array of points to be written to output file\n",
    "    :param outfilepath: specification of output file (format: las or laz)\n",
    "    :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    import laspy\n",
    "    hdr = laspy.LasHeader(version=\"1.4\", point_format=6)\n",
    "    hdr.x_scale = 0.00025\n",
    "    hdr.y_scale = 0.00025\n",
    "    hdr.z_scale = 0.00025\n",
    "    mean_extent = np.mean(outpoints, axis=0)\n",
    "    hdr.x_offset = int(mean_extent[0])\n",
    "    hdr.y_offset = int(mean_extent[1])\n",
    "    hdr.z_offset = int(mean_extent[2])\n",
    "\n",
    "    las = laspy.LasData(hdr)\n",
    "\n",
    "    las.x = outpoints[:, 0]\n",
    "    las.y = outpoints[:, 1]\n",
    "    las.z = outpoints[:, 2]\n",
    "    for key, vals in attribute_dict.items():\n",
    "        try:\n",
    "            las[key] = vals\n",
    "        except:\n",
    "            las.add_extra_dim(laspy.ExtraBytesParams(\n",
    "                name=key,\n",
    "                type=type(vals[0])\n",
    "            ))\n",
    "            las[key] = vals\n",
    "\n",
    "    las.write(outfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c82030-103d-408c-9bfc-3912af0fd5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': #The __name__ variable is used for when you want to import a function into another script\n",
    "                            #The __name__ var is by defauly '__main__', but if you are running a function from another script, __name__ is\n",
    "                            #Not entirely sure how this works in this case\n",
    "\n",
    "    use_columns = ['scan_angle_rank'] #Why using scan angle rank and not NormalizedZ?\n",
    "    #Here we specify the folder where the LAZ files are located\n",
    "    #Also specify the max number of points allowed in each cloud and the column names to add as additional input\n",
    "    train_dataset = PointCloudsInFiles(r'C:\\Users\\hseely\\OneDrive - UBC\\Documents\\Jupyter_Lab_Workspace\\Lukas_DL_Regression_Example\\train',\n",
    "                                       '*.laz',\n",
    "                                       'NormalizedZ', max_points=4_000, use_columns=use_columns)\n",
    "    test_dataset = PointCloudsInFiles(r'C:\\Users\\hseely\\OneDrive - UBC\\Documents\\Jupyter_Lab_Workspace\\Lukas_DL_Regression_Example\\test',\n",
    "                                      '*.laz',\n",
    "                                      'NormalizedZ', max_points=4_000, use_columns=use_columns)\n",
    "    #Use the DataLoader from torch geometric to load the training/test data, specifying batch size and number of workers, also shuffling data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                              num_workers=0) #Change to zero for my machine\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                             num_workers=0) #Change to zero for my machine\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Set device as cuda for parallel processing\n",
    "    print(f\"Using {device} device.\")\n",
    "    model = Net(num_features=len(use_columns)).to(device) #Specify the Net() class as the model which is described in the pn2_regressor.py script\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #Use the Adam optimizer, which is a stochastic gradient descent method\n",
    "        #Not sure what .parameters() is doing here\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(train_loader):#enumerate(): adds a counter (i, in this case) to an iterable and returns it in a form of enumerating object\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad() #Specify Adam optimizer function\n",
    "            out = model(data)\n",
    "            loss = F.mse_loss(out, data.y) #Specify mean squared error loss function, \n",
    "                                           #mse_loss() from TORCH.NN.FUNCTIONAL Measures the element-wise mean squared error.\n",
    "            #\n",
    "            loss.backward() #loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x.\n",
    "            optimizer.step() #Performs a single optimization step (parameter update)\n",
    "            if (i + 1) % 1 == 0:\n",
    "                print(f'[{i + 1}/{len(train_loader)}] MSE Loss: {loss.to(\"cpu\"):.4f} ')\n",
    "\n",
    "    #torch.no_grad(): Context-manager that disabled gradient calculation.useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "    @torch.no_grad() #@ is a decorator that extends the functionality of the no_grad() function\n",
    "    def test(loader, ep_id):\n",
    "        model.eval() #model.eval(): a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:\n",
    "        losses = []\n",
    "        for idx, data in enumerate(loader): \n",
    "            data = data.to(device)\n",
    "            outs = model(data)\n",
    "            loss = F.mse_loss(outs, data.y) #Computes loss\n",
    "            losses.append(float(loss.to(\"cpu\")))\n",
    "            if idx == 0:\n",
    "                batch = data.batch.to('cpu').numpy()\n",
    "                coords = data.pos.to('cpu').numpy()[batch==0, :]\n",
    "                vals = data.y.to('cpu').numpy()[batch==0, 0]\n",
    "                vals_pred = outs.to('cpu').numpy()[batch==0, 0]\n",
    "                write_las(coords, rf'C:\\Users\\hseely\\OneDrive - UBC\\Documents\\Jupyter_Lab_Workspace\\Lukas_DL_Regression_Example\\predicted\\ep{ep_id}_{idx}.laz',\n",
    "                          {'ref': vals,\n",
    "                           'pred': vals_pred\n",
    "                           } )\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "#Final loop to iterate through batch for each epoch, training, and then testing data to get change in MSE\n",
    "    for epoch in range(1, 501):\n",
    "        model_path = rf'C:\\Users\\hseely\\OneDrive - UBC\\Documents\\Jupyter_Lab_Workspace\\Lukas_DL_Regression_Example\\predicted\\latest.model'\n",
    "        if os.path.exists(model_path):\n",
    "            model = torch.load(model_path)\n",
    "        train() #What does empty train() function do? What is the input\n",
    "        mse = test(test_loader, epoch)\n",
    "        torch.save(model, model_path) #Save the model\n",
    "        print(f'Epoch: {epoch:02d}, Mean test MSE: {mse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
